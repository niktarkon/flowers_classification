{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport torch\nfrom PIL import Image\nimport tensorflow as tf\nimport datasets\nfrom torch.utils.data import Dataset as torch_ds\nimport torchvision.transforms as tfm\nimport re\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_addons as tfa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-17T19:02:52.062474Z","iopub.execute_input":"2023-02-17T19:02:52.063343Z","iopub.status.idle":"2023-02-17T19:03:00.941205Z","shell.execute_reply.started":"2023-02-17T19:02:52.063240Z","shell.execute_reply":"2023-02-17T19:03:00.940224Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2023-02-17 19:02:54.109678: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-17 19:02:54.109806: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"CONFIG = {\n    'seed': 42,\n    'buffer_size': 2048,\n    'device': 'tpu',\n    'root_directory': KaggleDatasets().get_gcs_path('tpu-getting-started'),\n    'lr':1e-5,\n    'batch_size_train': 4,\n    'batch_size_test': 20,\n    'parallel_setting': tf.data.experimental.AUTOTUNE,\n    'image_size': [192, 192],\n    'epochs':30,\n    'optimizer': 'adam',\n    'loss_function' : 'categorical_crossentropy'\n}\n\nCONFIG['images'] = ''.join([CONFIG['root_directory'], '/tfrecords-jpeg-512x512'])\nCONFIG['testing_filenames'] = tf.io.gfile.glob(CONFIG['images'] + '/test/*.tfrec')\n\n'''\nCONFIG['training_filenames'] = tf.io.gfile.glob(CONFIG['images_512x512'] + '/train/*.tfrec')\nCONFIG['validation_filenames'] = tf.io.gfile.glob(CONFIG['images_512x512'] + '/val/*.tfrec')\nCONFIG['testing_filenames'] = tf.io.gfile.glob(CONFIG['images_512x512'] + '/test/*.tfrec')\n'''\nCONFIG['classes'] = [   \n    'pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n    'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n    'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n    'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n    'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n    'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n    'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n    'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n    'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n    'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n    'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose'\n]  ","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:00.943539Z","iopub.execute_input":"2023-02-17T19:03:00.943930Z","iopub.status.idle":"2023-02-17T19:03:01.398543Z","shell.execute_reply.started":"2023-02-17T19:03:00.943887Z","shell.execute_reply":"2023-02-17T19:03:01.397667Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2023-02-17 19:03:01.314377: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(tf.io.gfile.glob(CONFIG['images_512x512'] + '/train/*.tfrec'))\nCONFIG['training_filenames'] = []\nCONFIG['validation_filenames'] = []\nfor fold in os.listdir('../input/tpu-getting-started')[1:]:\n    CONFIG['training_filenames'].extend(tf.io.gfile.glob(CONFIG['root_directory'] + '/' + fold + '/train/*.tfrec'))\n    CONFIG['validation_filenames'].extend(tf.io.gfile.glob(CONFIG['root_directory'] + '/' + fold + '/val/*.tfrec'))","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:01.400029Z","iopub.execute_input":"2023-02-17T19:03:01.401060Z","iopub.status.idle":"2023-02-17T19:03:02.008903Z","shell.execute_reply.started":"2023-02-17T19:03:01.401014Z","shell.execute_reply":"2023-02-17T19:03:02.007948Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2023-02-17 19:03:01.405653: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.483530: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.558152: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.638057: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.710868: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.781724: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.858340: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n2023-02-17 19:03:01.931082: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"len(CONFIG['training_filenames']), len(CONFIG['validation_filenames']), len(CONFIG['testing_filenames']), ","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:02.011056Z","iopub.execute_input":"2023-02-17T19:03:02.011325Z","iopub.status.idle":"2023-02-17T19:03:02.019919Z","shell.execute_reply.started":"2023-02-17T19:03:02.011285Z","shell.execute_reply":"2023-02-17T19:03:02.019103Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(64, 64, 16)"},"metadata":{}}]},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG['seed'])\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:02.021032Z","iopub.execute_input":"2023-02-17T19:03:02.021271Z","iopub.status.idle":"2023-02-17T19:03:07.154706Z","shell.execute_reply.started":"2023-02-17T19:03:02.021244Z","shell.execute_reply":"2023-02-17T19:03:07.153674Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2023-02-17 19:03:02.042961: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-17 19:03:02.045117: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-02-17 19:03:02.045170: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-02-17 19:03:02.045219: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ac32ab32b3a6): /proc/driver/nvidia/version does not exist\n2023-02-17 19:03:02.049083: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-17 19:03:02.050594: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","output_type":"stream"},{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2023-02-17 19:03:02.057155: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-02-17 19:03:02.088349: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-17 19:03:02.088407: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-02-17 19:03:02.106870: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-02-17 19:03:02.106934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-02-17 19:03:02.108876: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"class FlowersDataset(torch_ds):\n    def data_augment(self, image, label):\n        # Thanks to the dataset.prefetch(AUTO)\n        # statement in the next function (below), this happens essentially\n        # for free on TPU. Data pipeline code is executed on the \"CPU\"\n        # part of the TPU while the TPU itself is computing gradients.\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_saturation(image, 0, 2)\n        return image, label  \n    \n    def decode_image(self,image_data):\n        image = tf.image.decode_jpeg(image_data, channels=3)\n        image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n        image = tf.image.resize(image, CONFIG['image_size'])\n        image = tf.reshape(image, [*CONFIG['image_size'], 3]) # explicit size needed for TPU\n        return image\n\n    def read_labeled_tfrecord(self, example):\n        LABELED_TFREC_FORMAT = {\n            \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n            \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n        }\n        example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n        image = self.decode_image(example['image'])\n        label = tf.cast(example['class'], tf.int32)\n        return [image,  label] #{'image': image, 'label': label, 'idx' :idx} # returns a dataset of (image, label) pairs\n\n    def read_unlabeled_tfrecord(self, example):    \n        LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        }\n        example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n        image = self.decode_image(example['image'])\n        image = tf.cast(image, tf.float32)\n        idx = tf.cast(example['id'], tf.string) \n        return [image, idx] \n    \n    def load_dataset(self, filenames, labeled=True, ordered=False):\n        # Read from TFRecords. For optimal performance, reading from multiple files at once and\n        # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n        ignore_order = tf.data.Options()\n        if not ordered:\n            ignore_order.experimental_deterministic = False # disable order, increase speed\n\n        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=CONFIG['parallel_setting']) # automatically interleaves reads from multiple files\n        \n        \n        dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n        dataset = dataset.map(self.read_labeled_tfrecord if labeled else self.read_unlabeled_tfrecord, num_parallel_calls=CONFIG['parallel_setting'])\n        # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n        return dataset\n    \n    def __init__(self, train_filenames,\n            valid_filenames, test_filenames, path):\n        self.train_filenames = train_filenames\n        self.valid_filenames = valid_filenames\n        self.test_filenames = test_filenames\n        self.path = path\n\n    def save_dataset(self, path):\n        tf.data.experimental.save(self.ds, path)\n        \n    def load_processed_dataset(self, path):\n        return tf.data.experimental.load(path, reader_func=self.custom_reader_func)\n        \n    def create_datasets(self):\n        self.train_ds = self.load_dataset(self.train_filenames)\n        self.train_ds = self.train_ds.map(self.data_augment, num_parallel_calls=CONFIG['parallel_setting'])\n        self.val_ds = self.load_dataset(self.valid_filenames)\n        self.test_ds = self.load_dataset(self.test_filenames, labeled=False,ordered=False)\n    def apply_z_score_normalization(self): \n        dataset_filter = self.train_ds.map(lambda x1, x2: x1).as_numpy_iterator()\n        mean = 0.\n        std = 0.\n\n        cnt = 0\n        for np_array in dataset_filter:\n            mean += np_array.mean()\n            std += np_array.std()\n            cnt+=1\n        mean = mean/cnt\n        std = std/cnt \n        \n        self.train_ds = self.train_ds.map(lambda x1, x2,: ((x1 - mean)/std, x2))\n        self.val_ds = self.val_ds.map(lambda x1, x2: ((x1 - mean)/std, x2))\n        self.test_ds = self.test_ds.map(lambda x1, x2: ((x1 - mean)/std, x2))\n        \n    def create_batch_of_datasets(self):\n        self.train_ds = self.train_ds.map(lambda x, y: (x, tf.one_hot(y, depth=len(CONFIG['classes']))))\n        self.val_ds = self.val_ds.map(lambda x, y: (x, tf.one_hot(y, depth=len(CONFIG['classes']))))\n        \n        self.train_ds = self.train_ds.shuffle(buffer_size=CONFIG['buffer_size'], seed=CONFIG['seed'])\n        self.train_ds = self.train_ds.batch(CONFIG['batch_size_train'])\n        self.train_ds = self.train_ds.prefetch(CONFIG['parallel_setting']) \n        self.val_ds = self.val_ds.batch(CONFIG['batch_size_train'])\n        self.val_ds = self.val_ds.prefetch(CONFIG['parallel_setting'])\n        self.test_ds = self.test_ds.batch(CONFIG['batch_size_test'])\n        self.test_ds = self.test_ds.prefetch(CONFIG['parallel_setting'])","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:07.156444Z","iopub.execute_input":"2023-02-17T19:03:07.156820Z","iopub.status.idle":"2023-02-17T19:03:07.181793Z","shell.execute_reply.started":"2023-02-17T19:03:07.156774Z","shell.execute_reply":"2023-02-17T19:03:07.180417Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = FlowersDataset(CONFIG['training_filenames'],\n                         CONFIG['validation_filenames'],\n                         CONFIG['testing_filenames'], CONFIG['root_directory'])\ndataset.create_datasets()\ndataset.apply_z_score_normalization()\ndataset.create_batch_of_datasets()\ntrain_ds, val_ds, test_ds = dataset.train_ds, dataset.val_ds, dataset.test_ds","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:03:07.182941Z","iopub.execute_input":"2023-02-17T19:03:07.183177Z","iopub.status.idle":"2023-02-17T19:05:09.118361Z","shell.execute_reply.started":"2023-02-17T19:03:07.183150Z","shell.execute_reply":"2023-02-17T19:05:09.115999Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2023-02-17 19:05:08.921124: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 102056, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1676660708.917813922\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 102056, Output num: 0\",\"grpc_status\":3}\n","output_type":"stream"}]},{"cell_type":"code","source":"def exponential_lr(epoch,\n                   start_lr = 0.00001, min_lr = 0.00001, max_lr = 0.00005,\n                   rampup_epochs = 5, sustain_epochs = 0,\n                   exp_decay = 0.8):\n\n    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n        # linear increase from start to rampup_epochs\n        if epoch < rampup_epochs:\n            lr = ((max_lr - start_lr) /\n                  rampup_epochs * epoch + start_lr)\n        # constant max_lr during sustain_epochs\n        elif epoch < rampup_epochs + sustain_epochs:\n            lr = max_lr\n        # exponential decay towards min_lr\n        else:\n            lr = ((max_lr - min_lr) *\n                  exp_decay**(epoch - rampup_epochs - sustain_epochs) +\n                  min_lr)\n        return lr\n    return lr(epoch,\n              start_lr,\n              min_lr,\n              max_lr,\n              rampup_epochs,\n              sustain_epochs,\n              exp_decay)\n\nwith strategy.scope():\n    pretrained_model = tf.keras.applications.VGG16(\n        weights='imagenet',\n        include_top=False ,\n        input_shape=[*CONFIG['image_size'], 3]\n    )\n    \n    pretrained_model.trainable = False\n    \n    model = tf.keras.Sequential([\n        # To a base pretrained on ImageNet to extract features from images...\n        pretrained_model,\n        # ... attach a new head to act as a classifier.\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(len(CONFIG['classes']), activation='softmax')\n    ])\n    \n    f1_metric = tfa.metrics.F1Score(num_classes=len(CONFIG['classes']), threshold=0.5, average='micro')\n    \n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(exponential_lr, verbose=True)\n\nclass FlowerClassifier:\n    def __init__(self, model, metric, lr_callback, strategy, CONFIG):\n        self.model = model\n        self.metric = metric\n        self.lr_callback = lr_callback\n        self.strategy = strategy\n        \n        self.model.compile(\n            optimizer=CONFIG['optimizer'],\n            loss = CONFIG['loss_function'],\n            metrics=[self.metric],\n        )\n\n        self.model.summary()\n        \n    def fit(self, train_ds, val_ds):         \n        history = self.model.fit(\n            train_ds,\n            validation_data=val_ds,\n            epochs=CONFIG['epochs'],\n        )\n        \nflwrcls = FlowerClassifier(model, f1_metric,lr_callback, strategy, CONFIG)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T19:05:09.120445Z","iopub.execute_input":"2023-02-17T19:05:09.120814Z","iopub.status.idle":"2023-02-17T19:05:12.349022Z","shell.execute_reply.started":"2023-02-17T19:05:09.120772Z","shell.execute_reply":"2023-02-17T19:05:12.347516Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 0s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 6, 6, 512)         14714688  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 104)               53352     \n=================================================================\nTotal params: 14,768,040\nTrainable params: 53,352\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"flwrcls.fit(train_ds, val_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}